# ğŸ“Š Machine Learning with Python - Module 3: Classification and Regression

## ğŸ“– Overview

In **Module 3 â€“ Classification and Regression**, I explored various **supervised learning models** used to predict both **categorical** and **continuous values**. The focus was on:

- **Classification Models** â†’ Assigning categorical labels to data points.
- **Regression Models** â†’ Predicting continuous numerical values.
- **Decision Trees and Regression Trees** â†’ Rule-based learning for classification and regression.
- **Understanding Tree-Based Models** â†’ How they learn and make decisions.

This module introduced **structured decision-making algorithms**, improving upon simpler linear models, especially in handling **nonlinear relationships and hierarchical decision-making**.

---

## ğŸ“Œ Topics Covered

### 1ï¸âƒ£ **Classification: Categorizing Data into Groups**

Classification is a **supervised learning technique** where models categorize data points into **predefined labels**.

âœ” **Binary Classification** â†’ Two possible labels (e.g., spam vs. not spam).  
âœ” **Multiclass Classification** â†’ More than two categories (e.g., medical condition diagnosis).

#### **Multiclass Classification Strategies**

**1ï¸âƒ£ One-vs-All (OvA) Strategy**

- Trains **K binary classifiers**, each distinguishing **one class vs. all others**.
- The class with the **highest probability score** is assigned.  
  âœ” **Simple and efficient**, especially for high-dimensional data.

**2ï¸âƒ£ One-vs-One (OvO) Strategy**

- Trains **K(K-1)/2** classifiers, each distinguishing **two classes at a time**.
- A **voting mechanism** determines the final class label.  
  âœ” **More robust for similar classes**, but computationally expensive.

âœ” **Use OvA when the dataset is large and computing power is limited.**  
âœ” **Use OvO when classification accuracy is a priority over speed.**

---

### 2ï¸âƒ£ **Decision Trees: Rule-Based Learning**

Decision Trees are **hierarchical models** used for both **classification and regression**. They **split** the dataset into smaller **homogeneous** groups by applying **feature-based decisions** recursively.

#### **How Decision Trees Work**

- **Root Node** â†’ Represents the **entire dataset** and the **first splitting decision**.
- **Internal Nodes** â†’ Represent **feature-based decision rules**.
- **Branches** â†’ Represent the **outcome of decisions**.
- **Leaf Nodes** â†’ Represent the **final class assignment (classification)** or **predicted value (regression)**.

Each split is **chosen to maximize the homogeneity** of the resulting subsets. The tree grows **until a stopping condition is met**.

#### **Splitting Criteria in Decision Trees**

1ï¸âƒ£ **Entropy & Information Gain**  
âœ” **Entropy** measures **uncertainty** in a node.  
âœ” **Information Gain (IG)** measures how much entropy **decreases** after a split.

2ï¸âƒ£ **Gini Impurity**  
âœ” Measures **probability of incorrect classification**.  
âœ” **Gini is computationally more efficient** than entropy.

âœ” **Use Gini for speed, and Entropy for imbalanced datasets.**

#### **Preventing Overfitting: Pruning Decision Trees**

1ï¸âƒ£ **Pre-Pruning (Early Stopping)** â†’ Limits tree growth early.  
2ï¸âƒ£ **Post-Pruning (Reduced Error Pruning)** â†’ Removes unnecessary branches after training.

---

### 3ï¸âƒ£ **Regression Trees: Predicting Continuous Values**

Regression Trees extend Decision Trees to **predict numerical values** instead of categories.

#### **How Regression Trees Work**

âœ” The dataset is **split recursively** based on the **feature that minimizes prediction error**.  
âœ” The final prediction at each leaf node is the **average of the target values**.

#### **Split Criteria for Regression Trees**

âœ” **Mean Squared Error (MSE) Reduction** â†’ The feature that results in the lowest variance in the target variable is chosen for splitting.  
âœ” **Mean Absolute Error (MAE) Reduction** â†’ Similar to MSE but less sensitive to outliers.

âœ” **Regression Trees work well for nonlinear relationships but are sensitive to outliers.**

---

## **ğŸ“‚ Final Thoughts**

âœ… **Classification Trees and Regression Trees provide structured, rule-based predictions.**  
âœ… **Entropy, Gini, and MSE are used to evaluate splits and measure performance.**  
âœ… **Pruning is crucial for improving decision tree generalization.**  
âœ… **Scikit-learn provides easy-to-use implementations for Decision Trees and Regression Trees.**
