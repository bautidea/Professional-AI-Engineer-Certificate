# üìä Machine Learning with Python - Module 3: Other Supervised Learning Models

## üìñ Overview

In **Module 3 ‚Äì Other Supervised Learning Models**, I explored additional supervised learning techniques beyond decision trees. The focus was on:

- **K-Nearest Neighbors (KNN)** for instance-based classification.
- **Support Vector Machines (SVM)** for optimal decision boundary determination.
- **The Bias-Variance Tradeoff**, its impact on model performance, and techniques to mitigate it.
- **Ensemble Learning**, including **Bagging (Random Forests)** and **Boosting (XGBoost, AdaBoost)** to improve prediction accuracy.

This module introduced **more complex decision-making algorithms**, improving upon simpler linear models, especially in handling **nonlinear relationships and high-dimensional data**.

---

## üìå Topics Covered

### 1Ô∏è‚É£ **K-Nearest Neighbors (KNN): Instance-Based Learning**

K-Nearest Neighbors (**KNN**) is a **non-parametric**, **instance-based learning algorithm** used for **both classification and regression**.

‚úî **It does not explicitly learn a model during training**. Instead, it stores all training data and makes predictions **based on similarity**.

#### **How KNN Works**

1Ô∏è‚É£ **Choose a value for K** (the number of neighbors).  
2Ô∏è‚É£ **Calculate the distance** between the query point and all training data points.  
3Ô∏è‚É£ **Find the K-nearest points** to the query point.  
4Ô∏è‚É£ **For classification:** Assign the most common class among the K neighbors.  
5Ô∏è‚É£ **For regression:** Compute the average (or weighted average) of the K-nearest neighbors‚Äô values.

‚úî **KNN is effective for small datasets with low-dimensional data**.  
‚úî **Sensitive to noisy data** if K is too low.

---

### 2Ô∏è‚É£ **Support Vector Machines (SVM): Hyperplane-Based Classification**

SVM is a **supervised learning technique** used for **classification and regression**. It is particularly useful for **high-dimensional spaces** where a clear **decision boundary** is required.

‚úî **SVM finds the optimal hyperplane** that best separates two classes while maximizing the margin between them.

#### **Types of SVM Kernels**

- **Linear Kernel:** Used when data is linearly separable.
- **Polynomial Kernel:** Captures curved decision boundaries.
- **Radial Basis Function (RBF) Kernel:** Projects data into a higher-dimensional space to create better separation.
- **Sigmoid Kernel:** Similar to neural network activation functions.

‚úî **Use a linear kernel for simple datasets**.  
‚úî **Use RBF when working with complex, non-linearly separable data**.

---

### 3Ô∏è‚É£ **Bias-Variance Tradeoff**

The **Bias-Variance Tradeoff** is a fundamental concept in machine learning that describes how a model's complexity impacts its ability to **generalize to new data**. The goal of machine learning is to build a model that can **accurately predict outcomes on unseen data**, not just memorize training examples.

#### **What is Bias?**

Bias refers to **assumptions** a model makes about the relationship between input features and target labels. It is a measure of how far the model‚Äôs **predicted values** deviate from the actual values.

- **High Bias (Underfitting)**
  - The model is **too simple** and **fails to capture the complexity** of the data.
  - It makes **strong assumptions**, ignoring relationships between features.
  - **Consequences**:
    - The model performs **poorly on both training and test data**.
    - Produces **similar predictions** regardless of input variation.

‚úî **Low bias leads to more accurate predictions**.  
‚úñ **High bias leads to an oversimplified model that misses key patterns**.

#### **What is Variance?**

Variance refers to **how much a model‚Äôs predictions change** when trained on **different subsets of data**. A model with **high variance** is overly **sensitive to training data** and captures **random noise** instead of the underlying pattern.

- **High Variance (Overfitting)**
  - The model is **too complex** and **memorizes** the training data.
  - It captures **irrelevant noise**, rather than learning the actual pattern.
  - **Consequences**:
    - Performs **very well on training data** but **poorly on test data**.

‚úî **Low variance means the model is stable across different training datasets**.  
‚úñ **High variance causes models to be inconsistent and unreliable on new data**.

---

### **The Tradeoff: Balancing Bias and Variance**

| **Model Complexity**                   | **Bias** | **Variance** | **Performance on Training Data** | **Performance on Test Data** |
| -------------------------------------- | -------- | ------------ | -------------------------------- | ---------------------------- |
| **Simple Model (Underfitting)**        | üî¥ High  | ‚úÖ Low       | ‚ùå Poor                          | ‚ùå Poor                      |
| **Balanced Model (Good Fit)**          | ‚úÖ Low   | ‚úÖ Low       | ‚úÖ Good                          | ‚úÖ Good                      |
| **Overly Complex Model (Overfitting)** | ‚úÖ Low   | üî¥ High      | ‚úÖ Excellent                     | ‚ùå Poor                      |

‚úî **Goal:** Find the **optimal model complexity** where the model is **complex enough to learn from data** but **not too complex to memorize noise**.

---

### 4Ô∏è‚É£ **Ensemble Learning Techniques**

**Ensemble Learning** combines multiple models to create a more **robust, stable, and accurate** prediction system. Instead of relying on a **single model**, ensemble methods **combine the predictions of multiple models** to produce a **stronger final result**.

#### **Bagging (Bootstrap Aggregating)**

- **Definition:** Bagging is an ensemble technique that **reduces variance** by training multiple **independent models** on different **random subsets of data**.
- **How it Works:**
  1Ô∏è‚É£ Multiple copies of the training dataset are created using **random sampling with replacement (bootstrap sampling)**.  
  2Ô∏è‚É£ A separate model (e.g., Decision Tree) is trained on each subset.  
  3Ô∏è‚É£ Predictions from all models are **averaged (for regression)** or **majority-voted (for classification)**.

- **Example:** **Random Forests** (an ensemble of multiple decision trees using bagging).
- **Advantages:**  
  ‚úî Reduces **overfitting** by stabilizing predictions.  
  ‚úî Works well with **high-variance models** (like deep decision trees).  
  ‚úî Improves **model accuracy** by averaging multiple weak learners.

---

#### **Boosting**

- **Definition:** Boosting is an ensemble technique that **reduces bias** by **sequentially training weak models**, where each model **corrects the errors** of the previous model.
- **How it Works:**
  1Ô∏è‚É£ The first weak model is trained on the dataset.  
  2Ô∏è‚É£ The next model **focuses on examples misclassified by the previous model**.  
  3Ô∏è‚É£ This process repeats, progressively improving accuracy.  
  4Ô∏è‚É£ The final prediction is **a weighted sum** of all weak models.

- **Examples:**

  - **XGBoost** (Extreme Gradient Boosting)
  - **AdaBoost** (Adaptive Boosting)
  - **Gradient Boosting**

- **Advantages:**  
  ‚úî Reduces **bias**, making weak models stronger.  
  ‚úî Works well with **imbalanced datasets**.  
  ‚úî Often achieves **higher accuracy** than bagging methods.

---

#### **Random Forest vs. XGBoost**

| **Method**        | **Works By**                                                  | **Strengths**                               | **Weaknesses**               |
| ----------------- | ------------------------------------------------------------- | ------------------------------------------- | ---------------------------- |
| **Random Forest** | Uses **Bagging** to train multiple independent decision trees | Reduces **variance**, prevents overfitting  | Slower inference             |
| **XGBoost**       | Uses **Boosting** to sequentially correct weak learners       | Reduces **bias**, handles complex data well | Sensitive to hyperparameters |

‚úî **Bagging is used when variance is high (to stabilize predictions).**  
‚úî **Boosting is used when bias is high (to improve weak models).**  
‚úî **Both methods improve generalization performance.**

---
