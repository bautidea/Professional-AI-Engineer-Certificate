# ğŸ“Š Machine Learning with Python - Module 4: Building Unsupervised Learning Models - Clustering

## ğŸ“– Overview

In **Module 4**, I explored **unsupervised learning techniques** with a focus on **clustering methods**. These methods do not require labeled data but instead identify **patterns and structures** in datasets based on similarity.

Key topics covered in this module:  
âœ” **Clustering techniques** (Partition-based, Density-based, and Hierarchical).  
âœ” **K-Means clustering** and methods for determining the best K.  
âœ” **DBSCAN and HDBSCAN** for density-based clustering.  
âœ” **Hierarchical clustering** using agglomerative and divisive approaches.  
âœ” **Real-world applications** of clustering in **customer segmentation, anomaly detection, and pattern recognition**.

This module demonstrates **how clustering improves data analysis, dimensionality reduction, and feature selection**, making machine learning models more efficient and interpretable.

---

## ğŸ“Œ Topics Covered

### 1ï¸âƒ£ **What is Clustering?**

**Clustering** is an **unsupervised learning technique** that groups **similar data points** into clusters based on their features and relationships. Unlike classification, clustering does **not require labeled data**, meaning that patterns are found **without predefined categories**.

âœ” **Clusters are formed naturally** â†’ Data points that are close together in an **N-dimensional feature space** belong to the same group.  
âœ” **Identifies hidden structures** â†’ Useful in **customer segmentation, pattern recognition, and anomaly detection**.  
âœ” **Helps simplify large datasets** â†’ Reduces the complexity of datasets by summarizing them into representative clusters.

### **ğŸ“Œ Clustering Uses**

ğŸš€ Clustering is widely used for:  
âœ” **Customer segmentation** â†’ Grouping customers based on shopping habits.  
âœ” **Image segmentation** â†’ Identifying patterns in medical imaging (e.g., tumor detection).  
âœ” **Anomaly detection** â†’ Fraud detection or equipment failure prediction.  
âœ” **Feature selection** â†’ Identifying important attributes that define data patterns.  
âœ” **Data compression** â†’ Replacing raw data points with cluster representatives to reduce storage requirements.

---

## **2ï¸âƒ£ Types of Clustering Methods**

### **ğŸ“Œ Partition-Based Clustering (K-Means)**

Partition-based clustering divides the dataset into **K** clusters. Each data point is assigned to the closest centroid.  
âœ” **Efficient for large datasets** â†’ K-Means is computationally efficient.  
âœ” **Works well for well-separated clusters** â†’ Performs well when data points naturally form groups.

ğŸ“Œ **Limitations**  
âœ– Requires pre-defining **K** (number of clusters).  
âœ– Assumes clusters are **convex and equal in size**, which may not always be true.

---

### **ğŸ“Œ Density-Based Clustering (DBSCAN, HDBSCAN)**

Density-based clustering finds clusters based on **density regions**, rather than assuming a fixed number of clusters.  
âœ” **Works well for irregularly shaped clusters**.  
âœ” **Can detect noise and outliers**, unlike K-Means.  
âœ” **No need to specify K**.

ğŸ“Œ **Limitations**  
âœ– Struggles with clusters of **varying density**.  
âœ– Sensitive to **hyperparameter selection** (Îµ and MinPts).

---

### **ğŸ“Œ Hierarchical Clustering (Agglomerative & Divisive)**

Hierarchical clustering builds a **tree-like structure (dendrogram)** showing how clusters are related.

âœ” **Agglomerative Clustering** (Bottom-Up) â†’ Starts with individual points and merges them into clusters.  
âœ” **Divisive Clustering** (Top-Down) â†’ Starts with one large cluster and splits it into smaller clusters.

ğŸ“Œ **Limitations**  
âœ– **Computationally expensive** for large datasets.  
âœ– **Does not work well with noisy data**.

ğŸ“Œ **Distance Metrics for Hierarchical Clustering**  
Hierarchical clustering relies on **distance measures** to determine cluster similarity:  
âœ” **Single Linkage** â†’ Merges clusters based on the closest points.  
âœ” **Complete Linkage** â†’ Uses the farthest points for cluster merging.  
âœ” **Average Linkage** â†’ Uses the mean distance between clusters.

---

## **3ï¸âƒ£ K-Means Clustering: Centroid-Based Clustering**

âœ” **Divides data into K clusters** â†’ Each cluster is represented by a centroid.  
âœ” **Minimizes intra-cluster variance** â†’ Points assigned to the nearest centroid.

ğŸ“Œ **Algorithm Steps**  
1ï¸âƒ£ Select K random centroids.  
2ï¸âƒ£ Assign each point to the nearest centroid.  
3ï¸âƒ£ Recalculate centroids based on cluster members.  
4ï¸âƒ£ Repeat until centroids stop changing (convergence).

ğŸ“Œ **How to Determine K?**  
âœ” **Elbow Method** â†’ Plots intra-cluster variance vs. K. The **"elbow point"** indicates the best K value.  
âœ” **Silhouette Score** â†’ Measures cluster separation; higher values indicate better clustering.  
âœ” **Davies-Bouldin Index** â†’ Lower values indicate better clustering quality.

ğŸ“Œ **Challenges of K-Means**  
âœ– **Sensitive to initial centroids** â†’ Different initializations can lead to different results.  
âœ– **Assumes spherical clusters** â†’ May fail on irregularly shaped data.

ğŸ“Œ **Distance Metrics Used in K-Means**  
âœ” **Euclidean Distance** â†’ Default metric, assumes spherical clusters.  
âœ” **Manhattan Distance** â†’ Works better for grid-like data (e.g., city maps).  
âœ” **Cosine Distance** â†’ Used for text data and high-dimensional data.

---

## **4ï¸âƒ£ DBSCAN & HDBSCAN: Density-Based Clustering**

âœ” **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise)

- Uses **Îµ (radius)** and **MinPts (minimum points)** to define dense regions.
- **Finds arbitrary-shaped clusters**.
- **Identifies outliers** as noise points.

âœ” **HDBSCAN** (Hierarchical DBSCAN)

- Automatically adjusts **density thresholds** to find clusters.
- Works better with **varying densities** than DBSCAN.
- Creates a **hierarchical structure** for clusters.

ğŸ“Œ **Comparison of DBSCAN vs. HDBSCAN**  
| **Feature** | **DBSCAN** | **HDBSCAN** |
|-------------|------------|------------|
| Requires Îµ & MinPts? | âœ… Yes | âŒ No (automatically adjusts) |
| Handles Noise? | âœ… Yes | âœ… Yes |
| Works for Varying Densities? | âŒ No | âœ… Yes |
| Finds Arbitrary Shapes? | âœ… Yes | âœ… Yes |
| Performance | Faster | More computationally expensive |

ğŸš€ **Use DBSCAN** when clusters have **similar density** and the number of clusters is **unknown**.  
ğŸš€ **Use HDBSCAN** when clusters have **varying density** and require **automatic tuning**.

---

## **ğŸ“‚ Final Thoughts**

âœ… **Clustering finds hidden patterns** in data without requiring labels.  
âœ… **Different clustering methods** suit different data structures.  
âœ… **K-Means is efficient but assumes spherical clusters**.  
âœ… **DBSCAN/HDBSCAN are better for noisy, irregularly shaped clusters**.  
âœ… **Hierarchical clustering provides interpretable cluster relationships**.  
âœ… **Choosing K is critical for K-Means; use Elbow & Silhouette methods**.
