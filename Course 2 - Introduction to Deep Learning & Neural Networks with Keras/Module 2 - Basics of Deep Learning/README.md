# üß† Deep Learning with Keras - Module 2: Deep Learning Fundamentals

## üìñ Overview

Module 2 explores the foundational concepts that drive the training and optimization of neural networks. It covers:

- The mechanics of **gradient descent** and how models minimize prediction error.
- The **backpropagation** algorithm that enables neural networks to learn from mistakes.
- Challenges like the **vanishing gradient problem** and how activation functions affect learning.
- A practical breakdown of commonly used **activation functions** and when to apply them.

This module provides the core theoretical and algorithmic tools necessary for training deep learning models effectively.

---

## üìå Topics Covered

---

### 1Ô∏è‚É£ Gradient Descent

- **What is Gradient Descent?**
  - An iterative optimization algorithm used to **minimize the cost function**.
  - Core to training neural networks through **weight and bias adjustments**.
- **Key Elements**:
  - Cost Function: Quantifies error between prediction and ground truth.
  - Learning Rate: Controls the step size of updates.
  - Update Rule:
    \[
    new_weight = old_weight - learning_rate \* gradient
    \]
- **Behavior Around the Minimum**:
  - Large gradients ‚Üí big steps (faster convergence far from minimum).
  - Small gradients ‚Üí fine-tuning (slow near the minimum).
- **Learning Rate Impact**:
  - Too large: Overshooting, instability.
  - Too small: Slow convergence.

‚úÖ Gradient descent generalizes to deep networks, where thousands or millions of parameters are updated simultaneously to reduce error.

---

### 2Ô∏è‚É£ Backpropagation

- **What is Backpropagation?**
  - The algorithm that computes how each weight and bias contributed to the prediction error.
  - Uses the **chain rule** to compute gradients layer by layer.
- **Training Workflow**:

  1. Initialize weights and biases randomly.
  2. Perform **forward propagation** to compute predictions.
  3. Calculate error using a cost function.
  4. Apply **backpropagation** to compute gradients.
  5. Update parameters using gradient descent.
  6. Repeat across multiple inputs and epochs.

- **Gradient Calculation**:
  - Gradients are computed recursively through the network, starting from the output layer.
  - Each parameter is updated in proportion to its contribution to the overall error.

‚úÖ Enables deep networks to learn complex mappings from input to output through systematic, iterative learning.

---

### 3Ô∏è‚É£ Vanishing Gradient Problem

- **What is It?**
  - Gradients shrink exponentially as they propagate backward in deep networks.
  - Early layers receive near-zero gradients and stop learning.
- **Why It Happens**:
  - Activation functions like **sigmoid** and **tanh** produce small derivatives (‚â§ 0.25).
  - Repeated multiplication of small values during backpropagation leads to **vanishing gradients**.
- **Consequences**:
  - Early layers train very slowly or not at all.
  - Slower convergence and compromised model accuracy.
  - Learning becomes biased toward later layers.

‚úÖ This problem led to the rise of activation functions like **ReLU**, which maintain stronger gradient flow in deep networks.

---

### 4Ô∏è‚É£ Activation Functions

Activation functions introduce **non-linearity**, allowing neural networks to model complex, non-linear relationships between inputs and outputs.

#### üîß Common Activation Functions:

| Function       | Range           | Common Usage                       | Limitations / Notes                    |
| -------------- | --------------- | ---------------------------------- | -------------------------------------- |
| **Sigmoid**    | (0, 1)          | Historically used in hidden layers | Vanishing gradients; not zero-centered |
| **Tanh**       | (-1, 1)         | Hidden layers (less common now)    | Still suffers from vanishing gradients |
| **ReLU**       | [0, ‚àû)          | Standard for hidden layers         | "Dying ReLU" for negative inputs       |
| **Leaky ReLU** | (-‚àû, ‚àû)         | Variant of ReLU                    | Helps avoid dead neurons               |
| **Softmax**    | (0, 1), sum = 1 | Output layer (classification)      | Converts logits to probabilities       |

---

#### üîç Function Highlights

##### üî∏ Sigmoid

- Bounded output.
- Vanishing gradient for large inputs.
- Not symmetric around zero.
- Rarely used in hidden layers today.

##### üî∏ Tanh

- Symmetric version of sigmoid.
- Output centered around zero.
- Still prone to vanishing gradients in deep networks.

##### üî∏ ReLU

- Most widely used activation.
- Efficient, sparse activations.
- Avoids vanishing gradients for positive inputs.
- Not active for negative inputs (can lead to dead neurons).

##### üî∏ Softmax

- Used in **output layer** for multi-class classification.
- Converts raw scores into probability distribution over classes.

---

### 5Ô∏è‚É£ Choosing Activation Functions

- Use **ReLU** as the default for hidden layers.
- Avoid **sigmoid** and **tanh** in deep models unless required by architecture.
- Use **softmax** in the output layer for classification tasks.
- Consider alternatives like **Leaky ReLU** if ReLU causes inactive units.

---

## ‚úÖ Summary

This module provided a practical and mathematical foundation for training deep learning models. Key takeaways:

- **Gradient Descent** is the core optimization strategy for adjusting model parameters.
- **Backpropagation** distributes error through the network and enables parameter learning.
- The **vanishing gradient problem** affects learning in deep networks and is mitigated by better activation function choices.
- **ReLU** has become the standard in hidden layers for its computational efficiency and resistance to vanishing gradients.

These insights are essential before moving into building deeper architectures and working with real-world data in upcoming modules.
