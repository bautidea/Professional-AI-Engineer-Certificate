{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Building Advanced Transformers**\n",
    "\n",
    "**Estimated time needed:  30 minutes**  \n",
    "\n",
    "In this lab, you will implement and experiment with advanced Transformer models using Keras. \n",
    "\n",
    "**Learning objectives:** \n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Understand the core components of a Transformer architecture.\n",
    "- Implement a multi-head self-attention mechanism from scratch.\n",
    "- Train and evaluate a Transformer for time series prediction.\n",
    "- Handle preprocessing and scaling for time series data effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Transformer?\n",
    "\n",
    "The Transformer architecture was introduced in the paper *\"Attention Is All You Need\"*. It revolutionized natural language processing by using attention mechanisms instead of recurrence.\n",
    "\n",
    "### Key Components:\n",
    "- **Input Embedding:** Converts input tokens (or time steps) into vectors.\n",
    "- **Positional Encoding:** Injects information about the position of input tokens.\n",
    "- **Multi-Head Self-Attention:** Allows the model to focus on different parts of the input sequence.\n",
    "- **Feedforward Layers:** Process the attended information.\n",
    "- **Layer Normalization & Residual Connections:** Stabilize and speed up training.\n",
    "\n",
    "> Transformers are now widely used not only in NLP but also in time series forecasting, image recognition, and more.\n",
    "\n",
    "**Next:** You will implement parts of this architecture step-by-step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Instructions: \n",
    "\n",
    "### Step 1: Import necessary libraries \n",
    "\n",
    "Before you start, you need to import the required libraries: TensorFlow and Keras. Keras is included within TensorFlow as `tensorflow.keras.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting numpy<2.2.0,>=1.26.0 (from tensorflow)\n",
      "  Downloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.9-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.15.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (645.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.0/645.0 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading h5py-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Downloading markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.0.9-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.15.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (404 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, pyarrow, protobuf, optree, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.2.2 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.71.0 h5py-3.13.0 keras-3.10.0 libclang-18.1.1 markdown-3.8 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.0.9 numpy-2.1.3 opt-einsum-3.4.0 optree-0.15.0 protobuf-5.29.4 pyarrow-20.0.0 rich-14.0.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 termcolor-3.1.0 werkzeug-3.1.3 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m177.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (2.1.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.0 scikit-learn-1.6.1 scipy-1.15.3 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (104 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m148.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m146.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.0 kiwisolver-1.4.8 matplotlib-3.10.3 pillow-11.2.1 pyparsing-3.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow pyarrow \n",
    "%pip install pandas  \n",
    "%pip install scikit-learn \n",
    "%pip install matplotlib \n",
    "%pip install requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 11:31:30.465915: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-22 11:31:30.467806: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-22 11:31:30.473450: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-22 11:31:30.491996: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747913490.517170     300 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747913490.523810     300 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747913490.540750     300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747913490.540778     300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747913490.540780     300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747913490.540782     300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-22 11:31:30.546098: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Setup the Environment to generate synthetic stock price data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic stock_prices.csv created and loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a synthetic stock price dataset\n",
    "np.random.seed(42)\n",
    "data_length = 2000  # Adjust data length as needed\n",
    "trend = np.linspace(100, 200, data_length)\n",
    "noise = np.random.normal(0, 2, data_length)\n",
    "synthetic_data = trend + noise\n",
    "\n",
    "# Create a DataFrame and save as 'stock_prices.csv'\n",
    "data = pd.DataFrame(synthetic_data, columns=['Close'])\n",
    "data.to_csv('stock_prices.csv', index=False)\n",
    "print(\"Synthetic stock_prices.csv created and loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1899, 100, 1)\n",
      "Shape of Y: (1899,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "data = pd.read_csv('stock_prices.csv') \n",
    "data = data[['Close']].values \n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Prepare the data for training\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(len(data)-time_step-1):\n",
    "        a = data[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 100\n",
    "X, Y = create_dataset(data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "print(\"Shape of X:\", X.shape) \n",
    "print(\"Shape of Y:\", Y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`tensorflow` is the main library for machine learning in Python.  \n",
    "\n",
    "`stock_prices.csv` is the data set that is loaded. \n",
    "\n",
    "`MinMaxScaler` method is used to normalize the data.  \n",
    "\n",
    "`create_dataset`method is used to prepare the data for training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Multi-Head Self-Attention \n",
    "\n",
    "Define the Multi-Head Self-Attention mechanism. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    "\n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The MultiHeadSelfAttention layer implements the multi-head self-attention mechanism, which allows the model to focus on different parts of the input sequence simultaneously. \n",
    "\n",
    "- The attention parameter computes the attention scores and weighted sum of the values. \n",
    "\n",
    "- The split_heads parameter splits the input into multiple heads for parallel attention computation. \n",
    "\n",
    "- The call method applies the self-attention mechanism and combines the heads. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Transformer block \n",
    "\n",
    "Define the Transformer block. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code:\n",
    "\n",
    "- The TransformerBlock layer combines multi-head self-attention with a feed-forward neural network and normalization layers.  \n",
    "\n",
    "- Dropout is used to prevent overfitting. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network with residual connections and layer normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement Encoder Layer \n",
    "\n",
    "Define the Encoder layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(EncoderLayer, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The EncoderLayer is similar to the TransformerBlock but is a reusable layer in the Transformer architecture. \n",
    "\n",
    "- It consists of a MultiHeadSelfAttention mechanism followed by a feedforward neural network. \n",
    "\n",
    "- Both sub-layers have residual connections around them, and layer normalization is applied to the output of each sub-layer. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network, with residual connections and layer normalization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Implement Transformer encoder \n",
    "\n",
    "Define the Transformer Encoder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 11:42:40.005591: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout \n",
    "\n",
    "class MultiHeadSelfAttention(Layer): \n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    " \n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    "class TransformerBlock(Layer): \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n",
    "class TransformerEncoder(Layer): \n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerEncoder, self).__init__() \n",
    "        self.num_layers = num_layers \n",
    "        self.embed_dim = embed_dim \n",
    "        self.enc_layers = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)] \n",
    "        self.dropout = Dropout(rate) \n",
    "\n",
    "    def call(self, inputs, training=False): \n",
    "        x = inputs \n",
    "        for i in range(self.num_layers): \n",
    "            x = self.enc_layers[i](x, training=training) \n",
    "        return x \n",
    "\n",
    "# Example usage \n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "inputs = tf.random.uniform((1, 100, embed_dim)) \n",
    "outputs = transformer_encoder(inputs, training=False)  # Use keyword argument for 'training' \n",
    "print(outputs.shape)  # Should print (1, 100, 128) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The TransformerEncoder is composed of multiple TransformerBlock layers, implementing the encoding part of the Transformer architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build and Compile the Transformer model \n",
    "\n",
    "Integrate the Transformer Encoder into a complete model for sequential data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the necessary parameters \n",
    "\n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "outputs = tf.keras.layers.Dense(1)(flatten) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Summary of the model \n",
    "model.summary() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The Transformer Encoder model defines the necessary parameters, flattens the output, and ends with a dense layer to produce the final output.  \n",
    "\n",
    "- The model is then compiled with the Adam optimizer and mean squared error loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train the Transformer model \n",
    "\n",
    "Train the model on the prepared dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - loss: 13.9621 \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.2463 \n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.1694 \n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.1504 \n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.1211 \n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.1612 \n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.1250 \n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.1190 \n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - loss: 0.1076 \n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.1770 \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0970 \n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - loss: 0.0995 \n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0855 \n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 1s/step - loss: 0.1019 \n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.1892 \n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - loss: 0.0675 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - loss: 0.0590 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - loss: 0.0572 \n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - loss: 0.0577 \n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 1s/step - loss: 0.0976 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f8cac27b020>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The model is trained on the normalized stock price data for 20 epochs with a batch size of 32. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Evaluate and Make Predictions \n",
    "\n",
    "Evaluate the model's performance and make predictions on the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 322ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions \n",
    "predictions = model.predict(X) \n",
    "predictions = scaler.inverse_transform(predictions) \n",
    "\n",
    "# Prepare true values for comparison\n",
    "true_values = scaler.inverse_transform(data.reshape(-1, 1))\n",
    "\n",
    "# Plot the predictions vs true values\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(true_values, label='True Data') \n",
    "plt.plot(np.arange(time_step, time_step + len(predictions)), predictions, label='Predictions') \n",
    "plt.xlabel('Time') \n",
    "plt.ylabel('Stock Price') \n",
    "plt.legend() \n",
    "plt.title('Predictions vs True Data (Both Scaled Back)')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The model's predictions are transformed back to the original scale using the inverse transform of the scaler. \n",
    "\n",
    "- The true data and predictions are plotted to visualize the model's performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises: \n",
    "\n",
    " ### Exercise 1: Add dropout to the Transformer model \n",
    "\n",
    " **Objective: Understand how to add dropout layers to the Transformer model to prevent overfitting.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Add a dropout layer after the Flatten layer in the model. \n",
    "\n",
    "- Set the dropout rate to 0.5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - loss: 7.8003  \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 1s/step - loss: 1.4696 \n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - loss: 1.0192 \n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.4184 \n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.2391 \n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.1235 \n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0527 \n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0500 \n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0326 \n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0254 \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0207 \n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0197 \n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0174 \n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0146 \n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0148 \n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0194 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0167 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0158 \n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0136 \n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0161 \n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 309ms/step - loss: 0.0217\n",
      "Loss = 0.016200637444853783\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "dropout = Dropout(rate=0.5)(flatten)\n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    " \n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Loss = {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import Dropout \n",
    "\n",
    "  \n",
    "\n",
    "# Add a dropout layer after the Flatten layer \n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "\n",
    "dropout = Dropout(0.5)(flatten) \n",
    "\n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "  \n",
    "\n",
    "# Build the model \n",
    "\n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "  \n",
    "\n",
    "# Compile the model \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "  \n",
    "\n",
    "# Train the model \n",
    "\n",
    "model.fit(X, Y, epochs=20, batch_size=32) \n",
    "\n",
    "  \n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss: {loss}') \n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Experiment with different batch sizes \n",
    "\n",
    "**Objective: Observe the impact of different batch sizes on model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Train the model with a batch size of 16. \n",
    "\n",
    "- Train the model with a batch size of 64. \n",
    "\n",
    "- Compare the training time and performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 641ms/step - loss: 0.0189 \n",
      "Epoch 2/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 635ms/step - loss: 0.0228 \n",
      "Epoch 3/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 629ms/step - loss: 0.0323 \n",
      "Epoch 4/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 628ms/step - loss: 0.0267 \n",
      "Epoch 5/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 627ms/step - loss: 0.0170 \n",
      "Epoch 6/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 631ms/step - loss: 0.0194 \n",
      "Epoch 7/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 639ms/step - loss: 0.0303 \n",
      "Epoch 8/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 628ms/step - loss: 0.0213 \n",
      "Epoch 9/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 630ms/step - loss: 0.0241 \n",
      "Epoch 10/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 637ms/step - loss: 0.0211 \n",
      "Epoch 11/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 623ms/step - loss: 0.0176 \n",
      "Epoch 12/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 620ms/step - loss: 0.0262 \n",
      "Epoch 13/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 631ms/step - loss: 0.0177 \n",
      "Epoch 14/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 628ms/step - loss: 0.0171 \n",
      "Epoch 15/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 622ms/step - loss: 0.0185 \n",
      "Epoch 16/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 648ms/step - loss: 0.0141 \n",
      "Epoch 17/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 651ms/step - loss: 0.0143 \n",
      "Epoch 18/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 648ms/step - loss: 0.0167 \n",
      "Epoch 19/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 641ms/step - loss: 0.0192 \n",
      "Epoch 20/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 643ms/step - loss: 0.0298 \n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 326ms/step - loss: 0.0087\n",
      "batch_size=16 -> Loss = 0.009876256808638573\n",
      "Epoch 1/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - loss: 0.0186 \n",
      "Epoch 2/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - loss: 0.0073\n",
      "Epoch 3/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - loss: 0.0049\n",
      "Epoch 4/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - loss: 0.0047\n",
      "Epoch 5/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - loss: 0.0038\n",
      "Epoch 6/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - loss: 0.0038\n",
      "Epoch 7/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - loss: 0.0035\n",
      "Epoch 8/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - loss: 0.0039\n",
      "Epoch 9/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - loss: 0.0042\n",
      "Epoch 10/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - loss: 0.0036 \n",
      "Epoch 11/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - loss: 0.0038\n",
      "Epoch 12/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - loss: 0.0038 \n",
      "Epoch 13/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - loss: 0.0033 \n",
      "Epoch 14/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - loss: 0.0033\n",
      "Epoch 15/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - loss: 0.0029 \n",
      "Epoch 16/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - loss: 0.0035 \n",
      "Epoch 17/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - loss: 0.0029 \n",
      "Epoch 18/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - loss: 0.0032\n",
      "Epoch 19/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - loss: 0.0033 \n",
      "Epoch 20/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - loss: 0.0030\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 321ms/step - loss: 5.5289e-04\n",
      "batch_size=64 -> Loss = 0.00109025405254215\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'batch_size=16 -> Loss = {loss}')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'batch_size=64 -> Loss = {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Train the model with batch size 16\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss}')\n",
    "\n",
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Use a different activation function \n",
    "\n",
    " **Objective: Understand how different activation functions impact the model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Change the activation function of the Dense layer to `tanh`. \n",
    "\n",
    "- Train and evaluate the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - loss: 0.2976 \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.2943 \n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.2573 \n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.1399 \n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0500 \n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0088 \n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0054 \n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0043 \n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0034 \n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0030 \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0058 \n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0034 \n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - loss: 0.0041 \n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0030 \n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0038 \n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0035 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0027 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0027 \n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0020 \n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - loss: 0.0057 \n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 325ms/step - loss: 7.5956e-04\n",
      "Loss with tanh = 0.0006242574891075492\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "# Project the inputs to the embed_dim \n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    " \n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Loss with tanh = {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations on completing this lab! In this lab, you have built an advanced Transformer model using Keras and applied it to a time series forecasting task. You have learned how to define and implement multi-head self-attention, Transformer blocks, encoder layers, and integrate them into a complete Transformer model. By experimenting with different configurations and training the model, you can further improve its performance and apply it to various sequential data tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "8aae4de69f29de06e63c5f2d04ef24811d42d1553c8ac316f7ad75d55f2c2d79"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
