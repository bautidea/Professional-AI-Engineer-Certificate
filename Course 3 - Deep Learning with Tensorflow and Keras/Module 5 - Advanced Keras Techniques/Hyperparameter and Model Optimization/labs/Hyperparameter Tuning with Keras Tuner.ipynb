{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Hyperparameter Tuning with Keras Tuner**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to set up Keras Tuner and prepare the environment for hyperparameter tuning. \n",
    "\n",
    "## Learning objectives: \n",
    "By the end of this lab, you will: \n",
    "- Install Keras Tuner and import the necessary libraries\n",
    "- Load and preprocess the MNIST data set\n",
    "- Define a model-building function that uses hyperparameters to configure the model architecture\n",
    "- Set up Keras Tuner to search for the best hyperparameter configuration \n",
    "- Retrieve the best hyperparameters from the search and build a model with these optimized values\n",
    "\n",
    "## Prerequisites: \n",
    "- Basic understanding of Python programming \n",
    "- Keras and TensorFlow installed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Install the Keras Tuner \n",
    "\n",
    "This exercise guides you through the initial setup for using Keras Tuner. You install the library, import necessary modules, and load and preprocess the MNIST data set, which will be used for hyperparameter tuning. \n",
    "1. **Install Keras Tuner:**\n",
    "    - Use pip to install Keras Tuner\n",
    "2. **Import necessary libraries:**\n",
    "    - Import Keras Tuner, TensorFlow, and Keras modules\n",
    "3. **Load and preprocess the MNIST data set:**\n",
    "    - Load the MNIST data set.\n",
    "    - Normalize the data set by dividing by 255.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.16.2\n",
      "  Downloading tensorflow-2.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow==2.16.2)\n",
      "  Downloading absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow==2.16.2)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow==2.16.2)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow==2.16.2)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow==2.16.2)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow==2.16.2)\n",
      "  Downloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow==2.16.2)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.16.2)\n",
      "  Downloading ml_dtypes-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow==2.16.2)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.16.2)\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.16.2)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow==2.16.2)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.16.2)\n",
      "  Downloading grpcio-1.73.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow==2.16.2)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow==2.16.2)\n",
      "  Downloading keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting numpy<2.0.0,>=1.26.0 (from tensorflow==2.16.2)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.16.2) (0.45.1)\n",
      "Collecting rich (from keras>=3.0.0->tensorflow==2.16.2)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow==2.16.2)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow==2.16.2)\n",
      "  Downloading optree-0.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow==2.16.2)\n",
      "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow==2.16.2)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow==2.16.2)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.2) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.0.0->tensorflow==2.16.2)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.2)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (590.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.8/590.8 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.73.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Downloading markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.3.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.73.0 h5py-3.14.0 keras-3.10.0 libclang-18.1.1 markdown-3.8 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.1.0 numpy-1.26.4 opt-einsum-3.4.0 optree-0.16.0 protobuf-4.25.8 rich-14.0.0 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.2 termcolor-3.1.0 werkzeug-3.1.3 wrapt-1.17.2\n",
      "Collecting keras-tuner==1.4.7\n",
      "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: keras in /opt/conda/lib/python3.12/site-packages (from keras-tuner==1.4.7) (3.10.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from keras-tuner==1.4.7) (24.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from keras-tuner==1.4.7) (2.32.3)\n",
      "Collecting kt-legacy (from keras-tuner==1.4.7)\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner==1.4.7) (2.3.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner==1.4.7) (1.26.4)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner==1.4.7) (14.0.0)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner==1.4.7) (0.1.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner==1.4.7) (3.14.0)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner==1.4.7) (0.16.0)\n",
      "Requirement already satisfied: ml-dtypes in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner==1.4.7) (0.3.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->keras-tuner==1.4.7) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->keras-tuner==1.4.7) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->keras-tuner==1.4.7) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->keras-tuner==1.4.7) (2024.12.14)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/lib/python3.12/site-packages (from optree->keras->keras-tuner==1.4.7) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras->keras-tuner==1.4.7) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras->keras-tuner==1.4.7) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner==1.4.7) (0.1.2)\n",
      "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
      "Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n",
      "/bin/bash: line 1: 2.0.0: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.16.2\n",
    "!pip install keras-tuner==1.4.7\n",
    "!pip install numpy<2.0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: \n",
    "This code installs the necessary libraries using pip\n",
    "\n",
    "- **TensorFlow**: Ensures compatibility with the Keras Tuner.\n",
    "- **Keras Tuner**: The version used in this lab.\n",
    "- **Numpy**: Ensures compatibility with the other installed packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Increase recursion limit to prevent potential issues\n",
    "sys.setrecursionlimit(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: \n",
    "The sys.setrecursionlimit function is used to increase the recursion limit, which helps prevent potential recursion errors when running complex models with deep nested functions or when using certain libraries like TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 13:06:55.818240: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-15 13:06:55.819429: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-15 13:06:55.824042: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-15 13:06:55.837207: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-15 13:06:55.861442: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-15 13:06:55.861482: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-15 13:06:55.878028: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-15 13:06:56.966444: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Import necessary libraries\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress all Python warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set TensorFlow log level to suppress warnings and info messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all logs, 1 = filter out INFO, 2 = filter out INFO and WARNING, 3 = ERROR only\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "This code imports the necessary libraries:\n",
    "\n",
    "- **`keras_tuner`**: Used for hyperparameter tuning.\n",
    "- **`Sequential`**: A linear stack of layers in Keras.\n",
    "- **`Dense`**, **`Flatten`**: Common Keras layers.\n",
    "- **`mnist`**: The MNIST dataset, a standard dataset for image classification.\n",
    "- **`Adam`**: An optimizer in Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 28, 28)\n",
      "Validation data shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
    "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
    "\n",
    "print(f'Training data shape: {x_train.shape}')\n",
    "print(f'Validation data shape: {x_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "This code loads the MNIST dataset and preprocesses it:\n",
    "\n",
    "- **`mnist.load_data()`**: Loads the dataset, returning training and validation splits.\n",
    "- **`x_train / 255.0`**: Normalizes the pixel values to be between 0 and 1.\n",
    "- **`print(f'...')`**: Displays the shapes of the training and validation datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Defining the model with hyperparameters \n",
    "\n",
    "In this exercise, you define a model-building function that uses the `HyperParameters` object to specify the number of units in a dense layer and the learning rate. This function returns a compiled Keras model that is ready for hyperparameter tuning.\n",
    "\n",
    "**Define a model-building function:**\n",
    "- Create a function `build_model` that takes a `HyperParameters` object as input.\n",
    "- Use the `HyperParameters` object to define the number of units in a dense layer and the learning rate for the optimizer.\n",
    "- Compile the model with sparse categorical cross-entropy loss and Adam optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model-building function \n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "This function builds and compiles a Keras model with hyperparameters:\n",
    "\n",
    "- **`hp.Int('units', ...)`**: Defines the number of units in the Dense layer as a hyperparameter.\n",
    "- **`hp.Float('learning_rate', ...)`**: Defines the learning rate as a hyperparameter.\n",
    "- **`model.compile()`**: Compiles the model with the Adam optimizer and sparse categorical cross-entropy loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Configuring the hyperparameter search \n",
    "\n",
    "This exercise guides you through configuring Keras Tuner. You create a `RandomSearch` tuner, specifying the model-building function, the optimization objective, the number of trials, and the directory for storing results. The search space summary provides an overview of the hyperparameters being tuned. \n",
    "\n",
    "**Create a RandomSearch Tuner:**\n",
    "- Use the `RandomSearch` class from Keras Tuner. \n",
    "- Specify the model-building function, optimization objective (validation accuracy), number of trials, and directory for storing results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 2\n",
      "units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "learning_rate (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "# Create a RandomSearch Tuner \n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=4,\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir',\n",
    "    project_name='intro_to_kt'\n",
    ")\n",
    "\n",
    "# Display a summary of the search space \n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "This code sets up a Keras Tuner `RandomSearch`:\n",
    "\n",
    "- **`build_model`**: The model-building function.\n",
    "- **`objective='val_accuracy'`**: The metric to optimize (validation accuracy).\n",
    "- **`max_trials=10`**: The maximum number of different hyperparameter configurations to try.\n",
    "- **`executions_per_trial=2`**: The number of times to run each configuration.\n",
    "- **`directory='my_dir'`**: Directory to save the results.\n",
    "- **`project_name='intro_to_kt'`**: Name of the project for organizing results.\n",
    "\n",
    "Displays a summary of the hyperparameter search space, providing an overview of the hyperparameters being tuned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Running the hyperparameter search \n",
    "\n",
    "In this exercise, you run the hyperparameter search using the `search` method of the tuner. You provide the training and validation data along with the number of epochs. After the search is complete, the results summary displays the best hyperparameter configurations found. \n",
    "\n",
    "**Run the search:**\n",
    "- Use the `search` method of the tuner. \n",
    "- Pass in the training data, validation data, and the number of epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 05m 16s]\n",
      "val_accuracy: 0.9803500175476074\n",
      "\n",
      "Best val_accuracy So Far: 0.9803500175476074\n",
      "Total elapsed time: 00h 40m 01s\n",
      "Results summary\n",
      "Results in my_dir/intro_to_kt\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "units: 416\n",
      "learning_rate: 0.0012634328852360272\n",
      "Score: 0.9803500175476074\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "units: 416\n",
      "learning_rate: 0.001191702765239906\n",
      "Score: 0.9802500009536743\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "units: 480\n",
      "learning_rate: 0.00029843321050892146\n",
      "Score: 0.9788500070571899\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "units: 416\n",
      "learning_rate: 0.0045794428565260475\n",
      "Score: 0.9729999899864197\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "units: 256\n",
      "learning_rate: 0.00617050452258954\n",
      "Score: 0.9711500108242035\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "units: 160\n",
      "learning_rate: 0.00026330914240548723\n",
      "Score: 0.9702499806880951\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "units: 288\n",
      "learning_rate: 0.00013983216506567277\n",
      "Score: 0.9677500128746033\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "units: 128\n",
      "learning_rate: 0.00024074172861114117\n",
      "Score: 0.9675499796867371\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "units: 512\n",
      "learning_rate: 0.00942204396646721\n",
      "Score: 0.9639500081539154\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.009971878258105844\n",
      "Score: 0.9625499844551086\n"
     ]
    }
   ],
   "source": [
    "# Run the hyperparameter search \n",
    "tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val)) \n",
    "\n",
    "# Display a summary of the results \n",
    "tuner.results_summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "This command runs the hyperparameter search:\n",
    "\n",
    "- **`epochs=5`**: Each trial is trained for 5 epochs.\n",
    "- **`validation_data=(x_val, y_val)`**: The validation data to evaluate the model's performance during the search.\n",
    "\n",
    "After the search is complete, this command displays a summary of the best hyperparameter configurations found during the search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Analyzing and using the best hyperparameters \n",
    "\n",
    "In this exercise, you retrieve the best hyperparameters found during the search and print their values. You then build a model with these optimized hyperparameters and train it on the full training data set. Finally, you evaluate the model’s performance on the test set to ensure that it performs well with the selected hyperparameters. \n",
    "\n",
    "**Retrieve the best hyperparameters:**\n",
    "- Use the `get_best_hyperparameters` method to get the best hyperparameters. \n",
    "- Print the optimal values for the hyperparameters. \n",
    "\n",
    "**Build and train the model:**\n",
    "- Build a model using the best hyperparameters. \n",
    "- Train the model on the full training data set and evaluate its performance on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "The optimal number of units in the first dense layer is 416. \n",
      "\n",
      "The optimal learning rate for the optimizer is 0.0012634328852360272. \n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 14ms/step - accuracy: 0.8896 - loss: 0.3686 - val_accuracy: 0.9594 - val_loss: 0.1330\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - accuracy: 0.9728 - loss: 0.0883 - val_accuracy: 0.9716 - val_loss: 0.0922\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9831 - loss: 0.0545 - val_accuracy: 0.9754 - val_loss: 0.0843\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - accuracy: 0.9885 - loss: 0.0364 - val_accuracy: 0.9768 - val_loss: 0.0802\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9927 - loss: 0.0235 - val_accuracy: 0.9686 - val_loss: 0.1058\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - accuracy: 0.9932 - loss: 0.0206 - val_accuracy: 0.9764 - val_loss: 0.0868\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - accuracy: 0.9954 - loss: 0.0145 - val_accuracy: 0.9753 - val_loss: 0.0920\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - accuracy: 0.9938 - loss: 0.0166 - val_accuracy: 0.9751 - val_loss: 0.1088\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - accuracy: 0.9962 - loss: 0.0119 - val_accuracy: 0.9767 - val_loss: 0.0992\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - accuracy: 0.9963 - loss: 0.0107 - val_accuracy: 0.9784 - val_loss: 0.0969\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9745 - loss: 0.1092   \n",
      "Test accuracy: 0.979200005531311\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Retrieve the best hyperparameters \n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0] \n",
    "print(f\"\"\" \n",
    "\n",
    "The optimal number of units in the first dense layer is {best_hps.get('units')}. \n",
    "\n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}. \n",
    "\n",
    "\"\"\") \n",
    "\n",
    "# Step 2: Build and Train the Model with Best Hyperparameters \n",
    "model = tuner.hypermodel.build(best_hps) \n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.2) \n",
    "\n",
    "# Evaluate the model on the test set \n",
    "test_loss, test_acc = model.evaluate(x_val, y_val) \n",
    "print(f'Test accuracy: {test_acc}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "This code retrieves the best hyperparameters found during the search:\n",
    "\n",
    "- **`get_best_hyperparameters(num_trials=1)`**: Gets the best hyperparameter configuration.\n",
    "- **`print(f\"...\")`**: Prints the best hyperparameters.\n",
    "- **`model.fit(...)`**: Trains the model on the full training data with a validation split of 20%.\n",
    "- **`model.evaluate(...)`**: Evaluates the model on the test (validation) dataset and prints the accuracy, which gives an indication of how well the model generalizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice exercises \n",
    "\n",
    "### Exercise 1: Setting Up Keras Tuner \n",
    "\n",
    "#### Objective: \n",
    "Learn how to set up Keras Tuner and prepare the environment for hyperparameter tuning. \n",
    "\n",
    "#### Instructions: \n",
    "1. Install Keras Tuner.\n",
    "2. Import necessary libraries.\n",
    "3. Load and preprocess the MNIST data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train, x_test = x_train/255, x_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "!pip install keras-tuner \n",
    "\n",
    "# Step 2: Import necessary libraries \n",
    "import keras_tuner as kt \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "from tensorflow.keras.datasets import mnist \n",
    "from tensorflow.keras.optimizers import Adam \n",
    "\n",
    "# Step 3: Load and preprocess the MNIST data set \n",
    "(x_train, y_train), (x_val, y_val) = mnist.load_data() \n",
    "x_train, x_val = x_train / 255.0, x_val / 255.0 \n",
    "\n",
    "# Print the shapes of the training and validation datasets\n",
    "print(f'Training data shape: {x_train.shape}') \n",
    "print(f'Validation data shape: {x_val.shape}')\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Defining the model with hyperparameters \n",
    "\n",
    "#### Objective: \n",
    "Define a model-building function that uses hyperparameters to configure the model architecture. \n",
    "\n",
    "#### Instructions: \n",
    "1. Define a model-building function that uses the `HyperParameters` object to specify the number of units in a dense layer and the learning rate. \n",
    "2. Compile the model with sparse categorical cross-entropy loss and Adam optimizer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28,28)),\n",
    "        Dense(\n",
    "            units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "            activation='relu'\n",
    "        ),\n",
    "        Dense(\n",
    "            10,\n",
    "            activation='softmax'\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            learning_rate = hp.Float(\n",
    "               'learning_rate',\n",
    "                min_value=1e-3,\n",
    "                max_value=1e-2,\n",
    "                sampling='LOG'\n",
    "        )),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Step 1: Define a model-building function\n",
    "def build_model(hp):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Configuring the hyperparameter search \n",
    "\n",
    "#### Objective: \n",
    "Set up Keras Tuner to search for the best hyperparameter configuration. \n",
    "\n",
    "#### Instructions: \n",
    "1. Create a `RandomSearch` tuner using the model-building function. \n",
    "2. Specify the optimization objective, number of trials, and directory for storing results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=3,\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir',\n",
    "    project_name='ex3'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Step 1: Create a RandomSearch Tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,  # Ensure 'build_model' function is defined from previous code\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir',\n",
    "    project_name='intro_to_kt'\n",
    ")\n",
    "\n",
    "# Display a summary of the search space\n",
    "tuner.search_space_summary()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Running the hyperparameter search\n",
    "\n",
    "#### Objective: \n",
    "Run the hyperparameter search and dispaly the summary of the results.\n",
    "\n",
    "#### Instructions: \n",
    "1. Run the hyperparameter search using the `search` method of the tuner. \n",
    "2. Pass in the training data, validation data, and the number of epochs. \n",
    "3. Display a summary of the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 00m 53s]\n",
      "val_accuracy: 0.9592000246047974\n",
      "\n",
      "Best val_accuracy So Far: 0.9742499887943268\n",
      "Total elapsed time: 00h 06m 29s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=3,\n",
    "    validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Step 1: Run the hyperparameter search \n",
    "\n",
    "tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val)) \n",
    "\n",
    " # Display a summary of the results \n",
    "\n",
    "tuner.results_summary()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Analyzing and using the best hyperparameters \n",
    "\n",
    "#### Objective: \n",
    "Retrieve the best hyperparameters from the search and build a model with these optimized values. \n",
    "\n",
    "#### Instructions: \n",
    "1. Retrieve the best hyperparameters using the `get_best_hyperparameters` method. \n",
    "2. Build a model using the best hyperparameters. \n",
    "3. Train the model on the full training data set and evaluate its performance on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Optimal number of units = 128\n",
      "\n",
      "    Optimal Learning rate = 0.0010606668003897823\n",
      "\n",
      "Epoch 1/3\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.8693 - loss: 0.4649 - val_accuracy: 0.9516 - val_loss: 0.1648\n",
      "Epoch 2/3\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9591 - loss: 0.1383 - val_accuracy: 0.9678 - val_loss: 0.1100\n",
      "Epoch 3/3\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9752 - loss: 0.0846 - val_accuracy: 0.9688 - val_loss: 0.1068\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9663 - loss: 0.1118   \n",
      "Testr Acc = 0.9692000150680542\n"
     ]
    }
   ],
   "source": [
    "best_hyps = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "print(f'''\n",
    "    Optimal number of units = {best_hyps.get('units')}\\n\n",
    "    Optimal Learning rate = {best_hyps.get('learning_rate')}\n",
    "''')\n",
    "\n",
    "model = tuner.hypermodel.build(best_hyps)\n",
    "model.fit(x_train,y_train, epochs=3, validation_split=0.2)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Testr Acc = {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Step 1: Retrieve the best hyperparameters \n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0] \n",
    "\n",
    "print(f\"\"\" \n",
    "\n",
    "The optimal number of units in the first dense layer is {best_hps.get('units')}. \n",
    "\n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}. \n",
    "\n",
    "\"\"\") \n",
    "\n",
    " # Step 2: Build and train the model with best hyperparameters \n",
    "\n",
    "model = tuner.hypermodel.build(best_hps) \n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.2) \n",
    "\n",
    " # Evaluate the model on the validation set \n",
    "\n",
    "val_loss, val_acc = model.evaluate(x_val, y_val) \n",
    "\n",
    "print(f'Validation accuracy: {val_acc}') \n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "\n",
    "Congratulations on completing this lab! You have learned to set up Keras Tuner and prepare the environment for hyperparameter tuning. In addition, you defined a model-building function that uses hyperparameters to configure the model architecture. You configured Keras Tuner to search for the best hyperparameter configuration and learned to run the hyperparameter search and analyze the results. Finally, you retrieved the best hyperparameters and built a model with these optimized values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skillup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright ©IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "4480f294d0e5af91350ef1c70e7b3bd8f76b50e8822bcb90342d59ff1810e228"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
